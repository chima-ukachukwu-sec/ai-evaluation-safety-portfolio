# Project 1 â€” Adversarial AI Safety Testing & Data Exfiltration Risk

## Overview
Conducted adversarial red-team evaluations to assess large language model
susceptibility to privacy leakage, data exfiltration, and unsafe information reuse
across multi-turn interactions.

## Core Focus
- Multi-step adversarial testing
- Context accumulation risk
- Cross-turn memory pressure
- Privacy and data protection failures

## Abstracted Workflow
1. Establish benign or neutral conversational context
2. Introduce progressively risky elements across turns
3. Adapt phrasing to test robustness
4. Observe refusal behavior and boundary persistence
5. Classify outcomes (prevented, partial, failure)
6. Document patterns and failure modes

## Skills Demonstrated
- Adversarial AI testing
- Privacy & data leakage analysis
- Multi-turn reasoning evaluation
- Structured risk reporting

## NDA Notice
All scenarios are abstracted and simulated. No proprietary tactics,
prompts, or internal systems are included.
